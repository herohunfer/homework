{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "/home/herohunfer/anaconda2/envs/tensorflow2/bin/python /home/herohunfer/PycharmProjects/homework/mxnet-week2/train_mlp.py\n",
    "INFO:root:train-labels-idx1-ubyte.gz exists, skip to downloada\n",
    "INFO:root:train-images-idx3-ubyte.gz exists, skip to downloada\n",
    "INFO:root:t10k-labels-idx1-ubyte.gz exists, skip to downloada\n",
    "INFO:root:t10k-images-idx3-ubyte.gz exists, skip to downloada\n",
    "INFO:root:Epoch[0] Batch [100]\tSpeed: 922.70 samples/sec\taccuracy=0.112376\n",
    "INFO:root:Epoch[0] Batch [200]\tSpeed: 951.37 samples/sec\taccuracy=0.110900\n",
    "INFO:root:Epoch[0] Batch [300]\tSpeed: 961.93 samples/sec\taccuracy=0.114500\n",
    "INFO:root:Epoch[0] Batch [400]\tSpeed: 956.83 samples/sec\taccuracy=0.118400\n",
    "INFO:root:Epoch[0] Batch [500]\tSpeed: 953.67 samples/sec\taccuracy=0.120600\n",
    "INFO:root:Epoch[0] Train-accuracy=0.133131\n",
    "INFO:root:Epoch[0] Time cost=62.723\n",
    "INFO:root:Epoch[0] Validation-accuracy=0.137400\n",
    "INFO:root:Epoch[1] Batch [100]\tSpeed: 967.99 samples/sec\taccuracy=0.146931\n",
    "INFO:root:Epoch[1] Batch [200]\tSpeed: 995.99 samples/sec\taccuracy=0.174000\n",
    "INFO:root:Epoch[1] Batch [300]\tSpeed: 998.23 samples/sec\taccuracy=0.208200\n",
    "INFO:root:Epoch[1] Batch [400]\tSpeed: 896.43 samples/sec\taccuracy=0.243900\n",
    "INFO:root:Epoch[1] Batch [500]\tSpeed: 964.25 samples/sec\taccuracy=0.278200\n",
    "INFO:root:Epoch[1] Train-accuracy=0.312929\n",
    "INFO:root:Epoch[1] Time cost=63.241\n",
    "INFO:root:Epoch[1] Validation-accuracy=0.322200\n",
    "INFO:root:Epoch[2] Batch [100]\tSpeed: 837.78 samples/sec\taccuracy=0.336832\n",
    "INFO:root:Epoch[2] Batch [200]\tSpeed: 917.83 samples/sec\taccuracy=0.354100\n",
    "INFO:root:Epoch[2] Batch [300]\tSpeed: 867.91 samples/sec\taccuracy=0.361200\n",
    "INFO:root:Epoch[2] Batch [400]\tSpeed: 845.33 samples/sec\taccuracy=0.374000\n",
    "INFO:root:Epoch[2] Batch [500]\tSpeed: 914.73 samples/sec\taccuracy=0.385200\n",
    "INFO:root:Epoch[2] Train-accuracy=0.390202\n",
    "INFO:root:Epoch[2] Time cost=67.647\n",
    "INFO:root:Epoch[2] Validation-accuracy=0.397600\n",
    "INFO:root:Epoch[3] Batch [100]\tSpeed: 969.30 samples/sec\taccuracy=0.398515\n",
    "INFO:root:Epoch[3] Batch [200]\tSpeed: 902.16 samples/sec\taccuracy=0.403800\n",
    "INFO:root:Epoch[3] Batch [300]\tSpeed: 903.79 samples/sec\taccuracy=0.397500\n",
    "INFO:root:Epoch[3] Batch [400]\tSpeed: 926.08 samples/sec\taccuracy=0.405900\n",
    "INFO:root:Epoch[3] Batch [500]\tSpeed: 977.76 samples/sec\taccuracy=0.411700\n",
    "INFO:root:Epoch[3] Train-accuracy=0.410505\n",
    "INFO:root:Epoch[3] Time cost=64.886\n",
    "INFO:root:Epoch[3] Validation-accuracy=0.413200\n",
    "INFO:root:Epoch[4] Batch [100]\tSpeed: 784.62 samples/sec\taccuracy=0.409406\n",
    "INFO:root:Epoch[4] Batch [200]\tSpeed: 940.59 samples/sec\taccuracy=0.414700\n",
    "INFO:root:Epoch[4] Batch [300]\tSpeed: 870.32 samples/sec\taccuracy=0.401700\n",
    "INFO:root:Epoch[4] Batch [400]\tSpeed: 847.84 samples/sec\taccuracy=0.402600\n",
    "INFO:root:Epoch[4] Batch [500]\tSpeed: 860.71 samples/sec\taccuracy=0.414100\n",
    "INFO:root:Epoch[4] Train-accuracy=0.400707\n",
    "INFO:root:Epoch[4] Time cost=69.559\n",
    "INFO:root:Epoch[4] Validation-accuracy=0.402300\n",
    "INFO:root:Epoch[5] Batch [100]\tSpeed: 907.85 samples/sec\taccuracy=0.398317\n",
    "INFO:root:Epoch[5] Batch [200]\tSpeed: 953.86 samples/sec\taccuracy=0.393800\n",
    "INFO:root:Epoch[5] Batch [300]\tSpeed: 972.57 samples/sec\taccuracy=0.378200\n",
    "INFO:root:Epoch[5] Batch [400]\tSpeed: 945.42 samples/sec\taccuracy=0.377000\n",
    "INFO:root:Epoch[5] Batch [500]\tSpeed: 824.17 samples/sec\taccuracy=0.384500\n",
    "INFO:root:Epoch[5] Train-accuracy=0.356970\n",
    "INFO:root:Epoch[5] Time cost=65.635\n",
    "INFO:root:Epoch[5] Validation-accuracy=0.351400\n",
    "INFO:root:Epoch[6] Batch [100]\tSpeed: 907.58 samples/sec\taccuracy=0.341584\n",
    "INFO:root:Epoch[6] Batch [200]\tSpeed: 963.34 samples/sec\taccuracy=0.318000\n",
    "INFO:root:Epoch[6] Batch [300]\tSpeed: 947.25 samples/sec\taccuracy=0.289900\n",
    "INFO:root:Epoch[6] Batch [400]\tSpeed: 891.26 samples/sec\taccuracy=0.282500\n",
    "INFO:root:Epoch[6] Batch [500]\tSpeed: 961.72 samples/sec\taccuracy=0.279200\n",
    "INFO:root:Epoch[6] Train-accuracy=0.241919\n",
    "INFO:root:Epoch[6] Time cost=63.755\n",
    "INFO:root:Epoch[6] Validation-accuracy=0.222400\n",
    "INFO:root:Epoch[7] Batch [100]\tSpeed: 940.30 samples/sec\taccuracy=0.219010\n",
    "INFO:root:Epoch[7] Batch [200]\tSpeed: 913.96 samples/sec\taccuracy=0.191700\n",
    "INFO:root:Epoch[7] Batch [300]\tSpeed: 1001.33 samples/sec\taccuracy=0.153700\n",
    "INFO:root:Epoch[7] Batch [400]\tSpeed: 991.01 samples/sec\taccuracy=0.152500\n",
    "INFO:root:Epoch[7] Batch [500]\tSpeed: 961.97 samples/sec\taccuracy=0.148900\n",
    "INFO:root:Epoch[7] Train-accuracy=0.151818\n",
    "INFO:root:Epoch[7] Time cost=62.602\n",
    "INFO:root:Epoch[7] Validation-accuracy=0.183100\n",
    "INFO:root:Epoch[8] Batch [100]\tSpeed: 999.10 samples/sec\taccuracy=0.296931\n",
    "INFO:root:Epoch[8] Batch [200]\tSpeed: 834.75 samples/sec\taccuracy=0.627800\n",
    "INFO:root:Epoch[8] Batch [300]\tSpeed: 933.75 samples/sec\taccuracy=0.801100\n",
    "INFO:root:Epoch[8] Batch [400]\tSpeed: 1021.64 samples/sec\taccuracy=0.871600\n",
    "INFO:root:Epoch[8] Batch [500]\tSpeed: 1002.77 samples/sec\taccuracy=0.900100\n",
    "INFO:root:Epoch[8] Train-accuracy=0.906364\n",
    "INFO:root:Epoch[8] Time cost=63.182\n",
    "INFO:root:Epoch[8] Validation-accuracy=0.925200\n",
    "INFO:root:Epoch[9] Batch [100]\tSpeed: 979.16 samples/sec\taccuracy=0.928119\n",
    "INFO:root:Epoch[9] Batch [200]\tSpeed: 983.55 samples/sec\taccuracy=0.933000\n",
    "INFO:root:Epoch[9] Batch [300]\tSpeed: 879.41 samples/sec\taccuracy=0.936000\n",
    "INFO:root:Epoch[9] Batch [400]\tSpeed: 925.78 samples/sec\taccuracy=0.945600\n",
    "INFO:root:Epoch[9] Batch [500]\tSpeed: 976.06 samples/sec\taccuracy=0.943800\n",
    "INFO:root:Epoch[9] Train-accuracy=0.945354\n",
    "INFO:root:Epoch[9] Time cost=63.314\n",
    "INFO:root:Epoch[9] Validation-accuracy=0.950400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "\n",
    "def mlp_layer(input_layer, n_hidden, activation=None, BN=False):\n",
    "\n",
    "    \"\"\"\n",
    "    A MLP layer with activation layer and BN\n",
    "    :param input_layer: input sym\n",
    "    :param n_hidden: # of hidden neurons\n",
    "    :param activation: the activation function\n",
    "    :return: the symbol as output\n",
    "    \"\"\"\n",
    "\n",
    "    # get a FC layer\n",
    "    l = mx.sym.FullyConnected(data=input_layer, num_hidden=n_hidden)\n",
    "    # get activation, it can be relu, sigmoid, tanh, softrelu or none\n",
    "    if activation is not None:\n",
    "        l = mx.sym.Activation(data=l, act_type=activation)\n",
    "    if BN:\n",
    "        l = mx.sym.BatchNorm(l)\n",
    "    return l\n",
    "\n",
    "\n",
    "def get_mlp_sym():\n",
    "\n",
    "    \"\"\"\n",
    "    :return: the mlp symbol\n",
    "    \"\"\"\n",
    "\n",
    "    data = mx.sym.Variable(\"data\")\n",
    "    # Flatten the data from 4-D shape into 2-D (batch_size, num_channel*width*height)\n",
    "    data_f = mx.sym.flatten(data=data)\n",
    "\n",
    "    # Your Design\n",
    "    l = mlp_layer(input_layer=data_f, n_hidden=100, activation=\"relu\", BN=True)\n",
    "    l = mlp_layer(input_layer=l, n_hidden=100, activation=\"relu\", BN=True)\n",
    "    l = mlp_layer(input_layer=l, n_hidden=100, activation=\"relu\", BN=True)\n",
    "\n",
    "    # MNIST has 10 classes\n",
    "    l = mx.sym.FullyConnected(data=l, num_hidden=10)\n",
    "    # Softmax with cross entropy loss\n",
    "    mlp = mx.sym.SoftmaxOutput(data=l, name='softmax')\n",
    "    return mlp\n",
    "\n",
    "\n",
    "def conv_layer(input_layer, num_filter, activation=None, BN=False, pooling=False):\n",
    "    \"\"\"\n",
    "    :return: a single convolution layer symbol\n",
    "    \"\"\"\n",
    "    # todo: Design the simplest convolution layer\n",
    "    conv = mx.sym.Convolution(data=input_layer,\n",
    "                              num_filter=num_filter,\n",
    "                              kernel=(3,3),\n",
    "                              stride=(1,1),\n",
    "                              no_bias=True)\n",
    "\n",
    "    # Find the doc of mx.sym.Convolution by help command\n",
    "    if activation is not None:\n",
    "        conv = mx.sym.Activation(data = conv, act_type='relu')\n",
    "\n",
    "    # Do you need BatchNorm?\n",
    "    if BN:\n",
    "        conv = mx.sym.BatchNorm(conv)\n",
    "\n",
    "    # Do you need pooling?\n",
    "    # What is the expected output shape?\n",
    "    if pooling:\n",
    "        conv = mx.sym.Pooling(data=conv, stride=(1, 1), kernel=(3, 3), pool_type='max')\n",
    "    return conv\n",
    "\n",
    "\n",
    "# Optional\n",
    "def inception_layer():\n",
    "    \"\"\"\n",
    "    Implement the inception layer in week3 class\n",
    "    :return: the symbol of a inception layer\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_conv_sym():\n",
    "\n",
    "    \"\"\"\n",
    "    :return: symbol of a convolutional neural network\n",
    "    \"\"\"\n",
    "    data = mx.sym.Variable(\"data\")\n",
    "    # todo: design the CNN architecture\n",
    "    # Flatten the data from 4-D shape into 2-D (batch_size, num_channel*width*height)\n",
    "    #data_f = mx.sym.flatten(data=data)\n",
    "    # How deep the network do you want? like 4 or 5\n",
    "    l = conv_layer(input_layer=data, num_filter=32, activation=\"relu\", BN=False, pooling=True)\n",
    "    l = conv_layer(input_layer=l, num_filter=64, activation=\"relu\", BN=False, pooling=True)\n",
    "    l = conv_layer(input_layer=l, num_filter=128, activation=\"relu\", BN=False, pooling=True)\n",
    "    # How wide the network do you want? like 32/64/128 kernels per layer\n",
    "    # MNIST has 10 classes\n",
    "    fc = mx.sym.FullyConnected(data=l,num_hidden=10, no_bias=True)\n",
    "    # Softmax with cross entropy loss\n",
    "    conv = mx.sym.SoftmaxOutput(data=fc, name='softmax')\n",
    "    # How is the convolution like? Normal CNN? Inception Module? VGG like?\n",
    "    mx.viz.plot_network(conv)\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv = get_conv_sym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mx.viz.plot_network(conv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
